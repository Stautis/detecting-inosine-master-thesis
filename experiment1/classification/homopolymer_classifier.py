import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
import matplotlib.pyplot as plt

################################################################################
#
# This script reads a the output generated by the translate_data.py script and
# trains several models (SVM, Decision tree, kNN, Random Forest, GBM) on this 
# data. It prints confusion matrices and accuracy scores for all models. 
#
################################################################################

data = pd.read_csv("balanced_dataframe_w_sd_and_mean.csv")
data = data.drop(data.columns[[0,3]],axis=1)

y = data.base
x = data.drop(data.columns[[2]],axis=1)

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)

scaler = MinMaxScaler()
x_scale = scaler.fit_transform(x)

x_train_scale, x_test_scale, y_train_scale, y_test_scale = train_test_split(x_scale, y,test_size=0.2)

print("Running Classification!\n")
print("Running SVM first")
kernels = ['rbf']
decision=['ovr','ovo']
c_values = [1,10,100]
gamma_values = [10,100]
best_score = 0
for ker in kernels:
	for d in decision:
		for c in c_values:
			for g in gamma_values:
				print("Running with",ker,"kernel and c ="+str(c),"and gamma="+str(g)+". We are using",d,"decision function")
				rbf_class = svm.SVC(kernel=ker,gamma=g,C=c,decision_function_shape=d).fit(x_train_scale,y_train_scale)
				rbf_pred = rbf_class.predict(x_test_scale)
				score = rbf_class.score(x_test_scale,y_test_scale)
				print("got score:",score)
				print("confusion matrix:")
				print(confusion_matrix(y_test_scale,rbf_pred, labels=['I','T','G','C','A']))
				if score > best_score:
					best_clf = rbf_class

print("\nProceeding with Decision Tree")
decision_tree = DecisionTreeClassifier(max_depth=3,random_state=0).fit(x_train,y_train)
dt_pred = decision_tree.predict(x_test)
print("got score:",decision_tree.score(x_test,y_test))
print(confusion_matrix(y_test,dt_pred,labels=['I','T','G','C','A']))

print("\nProceeding with KNN")
n = [3,5,7,9]
for i in n:
	print("Currently running with n="+str(i))
	knn_class = KNeighborsClassifier(n_neighbors=i).fit(x_train,y_train)
	knn_pred = knn_class.predict(x_test)
	print("got score:",knn_class.score(x_test,y_test))
	print(confusion_matrix(y_test,knn_pred,labels=['I','T','G','C','A']))

print("\nProceeding with Random Forest")
rf_class = RandomForestClassifier(max_depth=3, random_state=0).fit(x_train,y_train)
rf_pred = rf_class.predict(x_test)
print("got score:",rf_class.score(x_test,y_test))
print(confusion_matrix(y_test,rf_pred,labels=['I','T','G','C','A']))

print("\nProceeding with GBM")
gbm_class = GradientBoostingClassifier(random_state=0).fit(x_train,y_train)
gbm_pred = gbm_class.predict(x_test)
print("got score:",gbm_class.score(x_test,y_test))
print(confusion_matrix(y_test,gbm_pred,labels=['I','T','G','C','A']))

'''
Can try to make this plot work, but will probably be very messy

ERROR: in cotourf, somehow gets y values, probably from Z?

x_min, x_max = x_scale[:, 0].min() - 1, x_scale[:, 0].max() + 1
y_min, y_max = x_scale[:, 1].min() - 1, x_scale[:, 1].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2),np.arange(y_min, y_max, 0.2))
print("xx",xx,"\nyy",yy)
plt.subplot(2, 2, 1)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

Z = best_clf.predict(np.c_[xx.ravel(), yy.ravel()])

Z = Z.reshape(xx.shape)
plt.contourf(xx,yy,Z,cmap=plt.cm.coolwarm,alpha=0.8)

plt.scatter(x_scale[:,0],x_scale[:,1],c=y,cmap=plt.cm.coolwarm)
plt.xlabel('mean')
plt.ylabel('sd')
plt.xlim(xx.min(),xx.max())
plt.ylim(yy.min(),yy.max())
plt.xticks()
plt.yticks()
plt.title("SVM Classificiation")
plt.show()
'''
